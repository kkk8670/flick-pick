import os
from pyspark.sql import SparkSession
from pyspark.ml.recommendation import ALS
from pyspark.sql.types import DoubleType
from pyspark.sql.functions import udf
from pyspark.sql.functions import explode, col, desc
from pyspark.ml.feature import HashingTF, IDF, Tokenizer
from pyspark.ml import Pipeline
import numpy as np




# Set environment variables
os.environ["HADOOP_HOME"] = "D:/download_app/hadoop"
os.environ["SPARK_HOME"] = (
    "C:\\Users\\Zelda\\AppData\\Roaming\\Python\\Python311\\site-packages\\pyspark"
)
os.environ["PYSPARK_PYTHON"] = "D:\\download_app\\anaconda\\envs\\start2\\python.exe"

# Initialize SparkSession
spark = (
    SparkSession.builder.appName("MovieRecommendation")
    .config("spark.hadoop.validateOutputSpecs", "false")
    .getOrCreate()
)
spark.sparkContext.setLogLevel("ERROR")


def load_data():
    """Load rating, movie, and tag datasets."""
    # åŠ è½½ ratings æ•°æ®ï¼ˆrating.csvï¼‰
    ratings_df = (
        spark.read.csv("../../data-smallest/ratings.csv", header=True, inferSchema=True)
        .select("userId", "movieId", "rating")
        .withColumn("userId", col("userId").cast("int"))
        .withColumn("movieId", col("movieId").cast("int"))
        .withColumn("rating", col("rating").cast("float"))
    )

    # åŠ è½½ ratingMovie æ•°æ®ï¼ˆratingMovie.csvï¼‰ï¼Œè¿™åŒ…å«äº†ç”µå½±çš„ title å’Œ genres ä¿¡æ¯
    movie_df = (
        spark.read.csv("../../data-smallest/ratingMovie.csv", header=True, inferSchema=True)
        .select("movieId", "title", "genres")
    )

    # åŠ è½½ tags æ•°æ®ï¼ˆtags.csvï¼‰
    tags_df = (
        spark.read.csv("../../data-smallest/tags.csv", header=True, inferSchema=True)
        .select("movieId", "tags_str")
    )

    return ratings_df, movie_df, tags_df



def build_base_als():
    """Define the base ALS model."""
    return ALS(
        userCol="userId",
        itemCol="movieId",
        ratingCol="rating",
        coldStartStrategy="drop",
    )


# def hyperparameter_optimization(ratings_df):
#     """Perform hyperparameter tuning for the ALS model."""
#     als = build_base_als()
#     param_grid = (
#         ParamGridBuilder()
#         .addGrid(als.rank, [10, 20, 50])
#         .addGrid(als.maxIter, [10, 20])
#         .addGrid(als.regParam, [0.01, 0.1, 1.0])
#         .build()
#     )
#     evaluator = RegressionEvaluator(metricName="rmse", labelCol="rating", predictionCol="prediction")
#     crossval = CrossValidator(estimator=als, estimatorParamMaps=param_grid, evaluator=evaluator, numFolds=3)
#     cv_model = crossval.fit(ratings_df)
#     return cv_model.bestModel

# test version
def hyperparameter_optimization(ratings_df):
    """Use default ALS model for quick testing."""
    als = ALS(
        userCol="userId",
        itemCol="movieId",
        ratingCol="rating",
        coldStartStrategy="drop",
        rank=10,          # é»˜è®¤å€¼
        maxIter=10,       # é»˜è®¤å€¼
        regParam=0.1      # é»˜è®¤å€¼
    )
    model = als.fit(ratings_df)
    return model

def cosine_similarity_udf(v1, v2):
    v1 = np.array(v1)  # å°† v1 è½¬æ¢ä¸º numpy æ•°ç»„
    v2 = np.array(v2)  # å°† v2 è½¬æ¢ä¸º numpy æ•°ç»„
    norm_v1 = np.linalg.norm(v1)  # è®¡ç®— v1 çš„èŒƒæ•°
    norm_v2 = np.linalg.norm(v2)  # è®¡ç®— v2 çš„èŒƒæ•°
    return float(np.dot(v1, v2) / (norm_v1 * norm_v2) if norm_v1 != 0 and norm_v2 != 0 else 0.0)

def weighted_recommendations(user_id, best_als_model, movie_df, tags_df, ratings_df):
    """ALS + å†…å®¹ç›¸ä¼¼åº¦èåˆæ’åºæ¨è"""
    # ğŸ¯ Step 1: ALS æ¨èå‰ 100 ä¸ªï¼ˆå¤šå–ä¸€ç‚¹ï¼‰
    user_recommendations = best_als_model.recommendForUserSubset(
        spark.createDataFrame([(user_id,)], ["userId"]), 100
    )
    user_rec_movies = user_recommendations.withColumn("rec", explode("recommendations")) \
        .select("userId", "rec.movieId", "rec.rating")

    # ğŸ§  Step 2: æ„å»º tags çš„ TF-IDF ç‰¹å¾
    tokenizer = Tokenizer(inputCol="tags_str", outputCol="words")
    hashing_tf = HashingTF(inputCol="words", outputCol="rawFeatures", numFeatures=100)
    idf = IDF(inputCol="rawFeatures", outputCol="features")
    pipeline = Pipeline(stages=[tokenizer, hashing_tf, idf])
    tag_model = pipeline.fit(tags_df)
    tags_df_transformed = tag_model.transform(tags_df)

    # ğŸ¯ Step 3: æ‰¾å‡ºç”¨æˆ·è¯„åˆ†è¿‡çš„ç”µå½±ï¼Œå–å…¶å†…å®¹ç‰¹å¾çš„å¹³å‡å‘é‡ä½œä¸º"ç”¨æˆ·å…´è¶£ç”»åƒ"
    user_history = ratings_df.filter(col("userId") == user_id)
    user_tag_features = user_history.join(tags_df_transformed, "movieId").select("features")

    # ç¼“å­˜ user_tag_features æ•°æ®é›†
    user_tag_features.cache()

    avg_vector = user_tag_features.rdd.map(lambda row: row["features"].toArray()).mean()
    avg_vector_broadcast = spark.sparkContext.broadcast(avg_vector)

    # ğŸ§  Step 4: å¯¹ ALS æ¨èåˆ—è¡¨çš„æ¯éƒ¨ç”µå½±å†…å®¹ä¸"ç”¨æˆ·å…´è¶£ç”»åƒ"åšä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—
    rec_with_features = user_rec_movies.join(tags_df_transformed, "movieId")

    # æ·»åŠ ä½™å¼¦ç›¸ä¼¼åº¦åˆ†æ•° - Moved this after avg_vector_broadcast is defined
    similarity_udf = udf(lambda v: cosine_similarity_udf(avg_vector_broadcast.value, v), DoubleType())
    rec_with_similarity = rec_with_features.withColumn("similarity", similarity_udf(col("features")))

    # ğŸ¯ Step 5: åŠ æƒè¯„åˆ†
    rec_with_score = rec_with_similarity.withColumn(
        "final_score", 0.7 * col("rating") + 0.3 * col("similarity")
    )

    # ğŸ¬ Step 6: æ‹¼æ¥ç”µå½±æ ‡é¢˜ä¿¡æ¯
    final_result = rec_with_score \
        .join(movie_df, "movieId", "left") \
        .select("movieId", "title", "rating", "similarity", "final_score") \
        .orderBy(desc("final_score"))

    print(f"\nğŸ¯ Top Recommendations for User {user_id}:\n")
    final_result.show(truncate=False)


def main():
    """Main function to load data, train models, and save the best model."""
    # åŠ è½½æ•°æ®
    ratings_df, movie_df, tags_df = load_data()  # åˆ é™¤äº† links_dfï¼Œå› ä¸ºä¸å†éœ€è¦å®ƒ

    # åˆ’åˆ†æ•°æ®é›†ï¼ˆ80% è®­ç»ƒï¼Œ20% æµ‹è¯•ï¼‰
    train_data, _ = ratings_df.randomSplit([0.8, 0.2], seed=42)

    # è®¾ç½®ç”¨æˆ· ID è¿›è¡Œæ¨èæµ‹è¯•
    user_id = 1  # ä¾‹å¦‚ï¼Œä½¿ç”¨ç”¨æˆ· ID = 1

    # è·å–æœ€ä½³æ¨¡å‹ï¼ˆALSæ¨¡å‹ï¼‰
    best_model = hyperparameter_optimization(train_data)

    # è·å–æ¨èç»“æœå¹¶æ‰“å°
    weighted_recommendations(user_id, best_model, movie_df, tags_df, ratings_df)

    # Windows ä¸‹æš‚ä¸ä¿å­˜æ¨¡å‹ï¼Œé¿å… Hadoop è·¯å¾„/æƒé™é—®é¢˜
    # save_path = "E:/prepare/se_rec/model"
    # os.makedirs(save_path, exist_ok=True)
    # best_model.write().overwrite().save(save_path)

    # ç»“æŸ Spark ä¼šè¯
    spark.stop()


if __name__ == "__main__":
    main()
